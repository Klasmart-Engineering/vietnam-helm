repositories:
  - name: bitnami
    url: https://charts.bitnami.com/bitnami

  - name: fluent
    url: https://fluent.github.io/helm-charts

  - name: prometheus
    url: https://prometheus-community.github.io/helm-charts

  - name: grafana
    url: https://grafana.github.io/helm-charts

  - name: oauth2-proxy
    url: https://oauth2-proxy.github.io/manifests

environments:
  vietnam-alpha:
    values:
      - ../../../env/vietnam-alpha/.env.yaml
  vietnam-beta:
    values:
      - ../../../env/vietnam-beta/.env.yaml
  vietnam-production:
    values:
      - ../../../env/vietnam-production/.env.yaml      
  indonesia-staging:
    values:
      - ../../../env/indonesia-staging/.env.yaml
  indonesia-production:
    values:
      - ../../../env/indonesia-production/.env.yaml
  indonesia-rk-prod:
    values:
      - ../../../env/indonesia-rk-prod/.env.yaml
  indonesia-rk-beta:
    values:
      - ../../../env/indonesia-rk-beta/.env.yaml
      
releases:

{{- if ne .Values.provider "gcp" }}

  - name: prometheus
    namespace: monitoring
    chart: bitnami/kube-prometheus
    version: ~3.0.1
    condition: helm_prometheus.enabled
    values:
      - global:
          storageClass: {{ .Values.persistentvolumeclaim_storage_class }}
      - operator:
          createCustomResource: false
      - prometheus:
          persistence:
            enabled: true
            storageClass: {{ .Values.persistentvolumeclaim_storage_class }}
            size: 8Gi
      - alertmanager:
          enabled: false
      - kubeProxy:
          enabled: false

  - name: fluentbit
    namespace: monitoring
    chart: fluent/fluent-bit
    condition: helm_fluentbit.enabled
    values:
      - extraVolumes:
          - name: aws-credentials
            secret:
              secretName: aws-credentials-fluentbit
      - extraVolumeMounts: 
          - name: aws-credentials
            mountPath: "/root/.aws"
            readOnly: true
      - config:
          outputs: >
            [OUTPUT]
                Name cloudwatch_logs
                Match *
                region ap-southeast-1
                log_group_name {{ .Values.fluentbit_cloudwatch_log_stream }}
                log_stream_prefix k8s-
{{- end }}

  - name: monitoring
    namespace: monitoring
    chart: ../charts/kube-monitoring
    version: ~0.1.0
    condition: helm_kube_monitoring.enabled
    values:
      # https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml
      - kube-prometheus-stack:
          alertmanager:
            config:
              global:
                # This is the webhook URL to our own kidsloop slack app
                slack_api_url: 'https://hooks.slack.com/services/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'
            alertmanagerSpec:
              routePrefix: /alertmanager/
              externalUrl: https://mon{{ .Values.kl_domain }}/alertmanager/
            templateFiles:
              # This file is a simple template to pass helm variables through to the alertmanager instance
              kidsloop.helm.tmpl: |-
                {{`{{ define "slack.kidsloop.channel" }}`}}{{ .Values.kube_monitoring.slack_channel }}{{`{{ end }}`}}

            ingress:
              enabled: true
              ingressClassName: nginx
              annotations:
                cert-manager.io/cluster-issuer: "letsencrypt"
                # https://kubernetes.github.io/ingress-nginx/examples/auth/basic/

                # Basic auth creds
                # nginx.ingress.kubernetes.io/auth-type: basic
                # # name of the secret that contains the user/password definitions
                # nginx.ingress.kubernetes.io/auth-secret: basic-auth
                # # message to display with an appropriate context why the authentication is required
                # nginx.ingress.kubernetes.io/auth-realm: 'mon{{ .Values.kl_domain }}'

                # OAuth config (mutually exclusive with basic auth)
                nginx.ingress.kubernetes.io/auth-url: "https://$host/oauth2/auth"
                nginx.ingress.kubernetes.io/auth-signin: "https://$host/oauth2/start?rd=$escaped_request_uri"
                # Other stuff for GKE LB
                kubernetes.io/ingress.allow-http: 'false'
                kubernetes.io/ingress.class: nginx
                kubernetes.io/ingress.global-static-ip-name: ingress-https-load-balancer
              labels: {}
              hosts:
                - mon{{ .Values.kl_domain }}
              paths:
                - /alertmanager/
              tls:
                - hosts:
                    - mon{{ .Values.kl_domain }}
                  secretName: mon-cert
          prometheus:
            prometheusSpec:
              externalUrl: https://mon{{ .Values.kl_domain }}/
              # externalUrl: https://mon{{ .Values.kl_domain }}/
              storageSpec:
                volumeClaimTemplate:
                  spec:
                    storageClassName: standard
                    accessModes: ["ReadWriteOnce"]
                    resources:
                      requests:
                        storage: 10Gi
            ingress:
              enabled: true
              ingressClassName: nginx
              annotations:
                cert-manager.io/cluster-issuer: "letsencrypt"
                # https://kubernetes.github.io/ingress-nginx/examples/auth/basic/
                # type of authentication
                # nginx.ingress.kubernetes.io/auth-type: basic
                # # name of the secret that contains the user/password definitions
                # nginx.ingress.kubernetes.io/auth-secret: basic-auth
                # # message to display with an appropriate context why the authentication is required
                # nginx.ingress.kubernetes.io/auth-realm: 'mon{{ .Values.kl_domain }}'
                # These are the OAuth params
                nginx.ingress.kubernetes.io/auth-url: "https://$host/oauth2/auth"
                nginx.ingress.kubernetes.io/auth-signin: "https://$host/oauth2/start?rd=$escaped_request_uri"
                # Other stuff for GKE LB
                kubernetes.io/ingress.allow-http: 'false'
                kubernetes.io/ingress.class: nginx
                kubernetes.io/ingress.global-static-ip-name: ingress-https-load-balancer
              labels: {}
              hosts:
                - mon{{ .Values.kl_domain }}
              paths:
                - /
              tls:
                - hosts:
                    - mon{{ .Values.kl_domain }}
                  secretName: mon-cert
          grafana:
            # https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
            ingress:
              enabled: true
              # ingressClassName: nginx
              annotations:
                cert-manager.io/cluster-issuer: "letsencrypt"
                # https://kubernetes.github.io/ingress-nginx/examples/auth/basic/
                # type of authentication
                # nginx.ingress.kubernetes.io/auth-type: basic
                # # name of the secret that contains the user/password definitions
                # nginx.ingress.kubernetes.io/auth-secret: basic-auth
                # # message to display with an appropriate context why the authentication is required
                # nginx.ingress.kubernetes.io/auth-realm: 'mon{{ .Values.kl_domain }}'
                # These are the OAuth params
                nginx.ingress.kubernetes.io/auth-url: "https://$host/oauth2/auth"
                nginx.ingress.kubernetes.io/auth-signin: "https://$host/oauth2/start?rd=$escaped_request_uri"
                # Other stuff for GKE LB
                kubernetes.io/ingress.allow-http: 'false'
                kubernetes.io/ingress.class: nginx
                kubernetes.io/ingress.global-static-ip-name: ingress-https-load-balancer
              labels: {}
              hosts:
                - mon{{ .Values.kl_domain }}
              path: /grafana/
              pathType: ImplementationSpecific
              paths:
                - /grafana/
              tls:
                - hosts:
                    - mon{{ .Values.kl_domain }}
                  secretName: mon-cert
            grafana.ini:
              server:
                domain: mon{{ .Values.kl_domain }}
                root_url: "https://%(domain)s/grafana"
                serve_from_sub_path: true
                enable_gzip: true
              users:
                auto_assign_org_role: Admin
              auth:
                oauth_auto_login: true
              auth.basic:
                enabled: false
              auth.generic_oauth:
                enabled: true
                client_id: ref+vault://{{ .Values.vault_path }}/monitoring/sso?address={{ .Values.vault_addr }}#/grafana_client_id
                client_secret: ref+vault://{{ .Values.vault_path }}/monitoring/sso?address={{ .Values.vault_addr }}#/grafana_client_secret
                scopes: openid email profile
                empty_scopes: false
                auth_url: https://dex.devops.klpsre.com/auth
                token_url: https://dex.devops.klpsre.com/token
                api_url: https://dex.devops.klpsre.com/userinfo
                allowed_domains: calmid.com kidsloop.live
                allow_sign_up: true
                tls_skip_verify_insecure: false
        oauth2-proxy:
          enabled: true
          # Oauth client configuration specifics
          config:
            # These are helmfile secrets (vals), see here on usage:
            # https://github.com/variantdev/vals#suported-backends
            # OAuth client ID
            clientID: ref+vault://{{ .Values.vault_path }}/monitoring/sso?address={{ .Values.vault_addr }}#/client_id
            # OAuth client secret
            clientSecret: ref+vault://{{ .Values.vault_path }}/monitoring/sso?address={{ .Values.vault_addr }}#/client_secret
            # Cookie encryption secret
            cookieSecret: ref+vault://{{ .Values.vault_path }}/monitoring/sso?address={{ .Values.vault_addr }}#/cookie_secret
            # Create a new secret with the following command
            # openssl rand -base64 32 | head -c 32 | base64
            # Use an existing secret for OAuth2 credentials (see secret.yaml for required fields)
            # Example:
            # existingSecret: secret
            # The name of the cookie that oauth2-proxy will create
            # If left empty, it will default to the release name
            cookieName: "ref+vault://{{ .Values.vault_path }}/monitoring/sso?address={{ .Values.vault_addr }}#/client_id"
            configFile: |-
              # email_domains = [ "*" ]
              upstreams = [ "file:///dev/null" ]
              http_address="0.0.0.0:4180"
              provider="oidc"
              email_domains = [ "calmid.com", "kidsloop.live" ]
              oidc_issuer_url="https://dex.devops.klpsre.com"
              cookie_secure="true"

          ingress:
            enabled: true
            path: /oauth2
            # Only used if API capabilities (networking.k8s.io/v1) allow it
            pathType: ImplementationSpecific
            # Used to create an Ingress record.
            hosts:
              - mon{{ .Values.kl_domain }}
            annotations:
              cert-manager.io/cluster-issuer: "letsencrypt"
              # Other stuff for GKE LB
              kubernetes.io/ingress.allow-http: 'false'
              kubernetes.io/ingress.class: nginx
              kubernetes.io/ingress.global-static-ip-name: ingress-https-load-balancer
            tls:
              # Secrets must be manually created in the namespace.
              - secretName: mon-cert
                hosts:
                  - mon{{ .Values.kl_domain }}

  - name: mysql-exporter
    namespace: okc
    chart: prometheus/prometheus-mysql-exporter
    version: "1.2.1"
    condition: helm_kube_monitoring.enabled
    values:
      # https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus-mysql-exporter/values.yaml
      - serviceMonitor:
          # enabled should be set to true to enable prometheus-operator discovery of this service
          enabled: true

        collectors:
          # auto_increment.columns: false
          binlog_size: true
          engine_innodb_status: true
          # engine_tokudb_status: false
          global_status: true
          # global_variables: true
          # info_schema.clientstats: false
          info_schema.innodb_metrics: true
          # info_schema.innodb_tablespaces: false
          # info_schema.innodb_cmp: false
          # info_schema.innodb_cmpmem: false
          # info_schema.processlist: false
          # info_schema.processlist.min_time: 0
          # info_schema.query_response_time: false
          # info_schema.tables: true
          # info_schema.tables.databases: '*'
          # info_schema.tablestats: false
          # info_schema.schemastats: false
          # info_schema.userstats: false
          # perf_schema.eventsstatements: false
          # perf_schema.eventsstatements.digest_text_limit: 120
          # perf_schema.eventsstatements.limit: false
          # perf_schema.eventsstatements.timelimit: 86400
          # perf_schema.eventswaits: false
          # perf_schema.file_events: false
          # perf_schema.file_instances: false
          # perf_schema.indexiowaits: false
          # perf_schema.tableiowaits: false
          # perf_schema.tablelocks: false
          # perf_schema.replication_group_member_stats: false
          # slave_status: true
          # slave_hosts: false
          # heartbeat: false
          # heartbeat.database: heartbeat
          # heartbeat.table: heartbeat

        # mysql connection params which build the DATA_SOURCE_NAME env var of the docker container
        mysql:
          db: "{{ .Values.mysql_database }}"
          host: "{{ .Values.mysql_proxy_ip}}"  # "{{ .Values.mysql_host }}"  # {{ .Values.mysql_proxy_ip}} ?
          param: "parseTime=true&charset=utf8mb4"
          pass: "password"
          port: 3306
          # protocol: ""
          user: "{{ .Values.mysql_username }}"
          # secret with full DATA_SOURCE_NAME env var as stringdata
          # existingSecret: ""
          # secret only containing the password
          existingPasswordSecret:
            name: mysql
            key: mysql-password

  - name: postgres-exporter
    namespace: okc
    chart: prometheus/prometheus-postgres-exporter
    version: "2.3.5"
    condition: helm_kube_monitoring.enabled
    values:
      # https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus-postgres-exporter/values.yaml
      - serviceMonitor:
          # enabled should be set to true to enable prometheus-operator discovery of this service
          enabled: true

        prometheusRule:
          enabled: true
          additionalLabels: {}
          namespace: "okc"
          rules:
            - alert: PSQLConnectUsedHighPct
              expr: sum(pg_stat_activity_count) / min(pg_settings_max_connections) > 0.8
              for: 1s
              labels:
                severity: critical
              annotations:
                description: Over 80% of available connections are in use
                summary: Over 80% of available connections are in use
                dashboard: https://mon{{ .Values.kl_domain }}/grafana/d/postgres/postgresql?orgId=1&refresh=5s
                runbook_url: https://help.compose.com/docs/postgresql-connection-limits

            - alert: PSQLConnectUsedHighWatermark
              expr: min(pg_settings_max_connections) - sum(pg_stat_activity_count) < 5
              for: 1s
              labels:
                severity: critical
              annotations:
                description: There are less than 5 connection slots available in postgres
                summary: There are less than 5 connection slots available in postgres
                dashboard: https://mon{{ .Values.kl_domain }}/grafana/d/postgres/postgresql?orgId=1&refresh=5s
                runbook_url: https://help.compose.com/docs/postgresql-connection-limits

        config:
          datasource:
            # Specify one of both datasource or datasourceSecret
            {{- if eq .Values.provider "gcp" }}
            host: "127.0.0.1"
            {{- else }}
            host: {{ .Values.postgresql_host }}
            {{- end }}
            user: {{ .Values.postgresql_username }}
            # Only one of password and passwordSecret can be specified
            # password: 
            # Specify passwordSecret if DB password is stored in secret.
            passwordSecret:
              # Secret name
              name: postgresql
              # Password key inside secret
              key: postgresql-password
            port: "5432"
            database: '{{ .Values.postgresql_database }}'
            sslmode: disable
          datasourceSecret: {}
            # Specifies if datasource should be sourced from secret value in format: postgresql://login:password@hostname:port/dbname?sslmode=disable
            # Multiple Postgres databases can be configured by comma separated postgres connection strings
            # Secret name
            #  name:
            # Connection string key inside secret
            #  key:
        # mysql connection params which build the DATA_SOURCE_NAME env var of the docker container
        {{- if eq .Values.provider "gcp" }}
        extraContainers:
        - name: cloud-sql-proxy
          image: gcr.io/cloudsql-docker/gce-proxy:1.19.1
          command:
            - "/cloud_sql_proxy"
            - "-ip_address_types=PRIVATE"
            - "-instances={{ .Values.terraform_project }}:{{ .Values.terraform_region }}:{{ .Values.postgresql_database }}=tcp:5432"
          securityContext:
            runAsNonRoot: true
        serviceAccount:
          # Specifies whether a ServiceAccount should be created
          create: false
          # The name of the ServiceAccount to use.
          # If not set and create is true, a name is generated using the fullname template
          name: cloudsql-proxy
          # Add annotations to the ServiceAccount, useful for EKS IAM Roles for Service Accounts or Google Workload Identity.
        {{- end }}

  - name: redis-exporter
    namespace: okc
    chart: prometheus/prometheus-redis-exporter
    version: "4.2.0"
    condition: helm_kube_monitoring.enabled
    values:
      # https://github.com/prometheus-community/helm-charts/blob/main/charts/prometheus-redis-exporter/values.yaml
      - redisAddress: redis://{{ .Values.redis_host }}:6379
        auth:
          # Use password authentication
          enabled: false
          # Use existing secret (ignores redisPassword)
          secret:
            name: ""
            key: ""
          # Redis password (when not stored in a secret)
          redisPassword: ""
        serviceMonitor:
          # When set true then use a ServiceMonitor to configure scraping
          enabled: true

        ## Custom PrometheusRules to be defined
        ## The value is evaluated as a template, so, for example, the value can depend on .Release or .Chart
        ## ref: https://github.com/coreos/prometheus-operator#customresourcedefinitions
        prometheusRule:
          enabled: false
          additionalLabels: {}
          namespace: ""
          rules: []
