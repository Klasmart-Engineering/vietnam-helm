oauth2-proxy:
  enabled: false

  ingress:
    enabled: true
    path: /oauth2
    # Only used if API capabilities (networking.k8s.io/v1) allow it
    pathType: ImplementationSpecific
    # Used to create an Ingress record.

  metrics:
    # Enable Prometheus metrics endpoint
    enabled: true
    # Serve Prometheus metrics on this port
    port: 44180
    servicemonitor:
      # Enable Prometheus Operator ServiceMonitor
      enabled: true
      # Define the namespace where to deploy the ServiceMonitor resource
      namespace: ""
      # Prometheus Instance definition
      prometheusInstance: default
      # Prometheus scrape interval
      interval: 60s
      # Prometheus scrape timeout
      scrapeTimeout: 30s
      # Add custom labels to the ServiceMonitor resource
      labels: {}

kube-prometheus-stack:
  # https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml
  enabled: false
  prometheus:
    prometheusSpec:

      ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the
      ## prometheus resource to be created with selectors based on values in the helm deployment,
      ## which will also match the PrometheusRule resources created
      ruleSelectorNilUsesHelmValues: false
      serviceMonitorSelectorNilUsesHelmValues: false
      podMonitorSelectorNilUsesHelmValues: false
      probeSelectorNilUsesHelmValues: false
  grafana:
    # https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
    additionalDataSources:
      - name: Loki
        url: http://monitoring-loki.monitoring.svc.cluster.local:3100
        orgId: 1
        type: loki

    plugins:
      - redis-datasource

    # Load the sre-standard dashboards into grafana
    dashboardsConfigMaps:
      sre-standard: "monitoring-grafana-dashboards-sre-standard"  

    serviceMonitor:
      enabled: true
      interval: 1m
    persistence:
      type: pvc
      enabled: true
      # storageClassName: gp2
      accessModes:
        - ReadWriteOnce
      size: 10Gi
      # annotations: {}
      finalizers:
        - kubernetes.io/pvc-protection
      # selectorLabels: {}
      # subPath: ""
      # existingClaim:

    dashboardProviders:
      dashboardproviders.yaml:
        apiVersion: 1
        providers:
        - name: 'sre-standard'
          orgId: 1
          folder: ''
          type: file
          disableDeletion: true
          editable: false
          options:
            path: /var/lib/grafana/dashboards/sre-standard

    # dashboards:
    #   default:
        # some-dashboard:
        #   json: |
        #     $RAW_JSON
        # pod_dash:
        #   file: dashboards/pod_dash.json
        # prometheus-stats:
        #   gnetId: 2
        #   revision: 2
        #   datasource: Prometheus
        # local-dashboard:
        #   url: https://example.com/repository/test.json
        #   token: ''
        # local-dashboard-base64:
        #   url: https://example.com/repository/test-b64.json
        #   token: ''
        #   b64content: true

    grafana.ini:
      # Customise settings in the grafana.ini file
      # https://grafana.com/docs/grafana/latest/administration/configuration/
      # server:
      #   # The full public facing url you use in browser, used for redirects and emails
      #   root_url: 

    ingress:
      enabled: false
      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
      # ingressClassName: nginx
      # Values can be templated
      annotations:
        kubernetes.io/ingress.class: nginx
        cert-manager.io/cluster-issuer: letsencrypt
      labels:
        app: grafana
      path: /
      # pathType is only for k8s >= 1.1=
      pathType: Prefix
      hosts:
        - grafana.devops.klpsre.com
      tls:
        - secretName: grafana-tls
          hosts:
            - grafana.devops.klpsre.com

  alertmanager:
    config:
      global:
        resolve_timeout: 5m
        # This is the webhook URL to our own kidsloop slack app
        slack_api_url: 'https://hooks.slack.com/services/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'
      route:
        group_by: ['job']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 12h
        receiver: 'slack-notifications'
        routes:
        - match:
            alertname: Watchdog
          receiver: 'null'
      receivers:
        - name: 'null'
        - name: 'slack-notifications'
          slack_configs:
            - channel: '{{ template "slack.kidsloop.channel" . }}'
              send_resolved: true
              icon_url: https://avatars3.githubusercontent.com/u/3380462
              title: '{{ template "slack.monzo.title" . }}'
              # icon_emoji: '{{ template "slack.monzo.icon_emoji" . }}'
              color: '{{ template "slack.kidsloop.color" . }}'
              text: '{{ template "slack.kidsloop.text" . }}'
              actions:
              - type: button
                text: 'Runbook :green_book:'
                url: '{{ (index .Alerts 0).Annotations.runbook }}'
              - type: button
                text: 'Query :mag:'
                url: '{{ (index .Alerts 0).GeneratorURL }}'
              - type: button
                text: 'Dashboard :tv:'
                url: '{{ (index .Alerts 0).Annotations.dashboard }}'
              - type: button
                text: 'Silence :no_bell:'
                url: '{{ template "__alert_silence_link" . }}'
              - type: button
                text: '{{ template "slack.monzo.link_button_text" . }}'
                url: '{{ .CommonAnnotations.link_url }}'
      templates:
        - '/etc/alertmanager/config/*.tmpl'
        - '/etc/alertmanager/templates/*.tmpl'

    alertmanagerSpec:
      # Set up proper storage
      storage:
        volumeClaimTemplate:
          spec:
            storageClassName: standard
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 2Gi

      # Additional volumes on the output StatefulSet definition.
      volumes:
        - name: template-volume
          configMap:
            name: alertmanager-templates-cm
      # Additional VolumeMounts on the output StatefulSet definition.
      volumeMounts:
        - name: template-volume
          mountPath: /etc/alertmanager/templates
          readOnly: true

loki-stack:
  # https://github.com/grafana/helm-charts/blob/main/charts/loki-stack/values.yaml
  enabled: true
  loki:
    enabled: true
    # https://github.com/grafana/helm-charts/blob/main/charts/loki/values.yaml
    serviceMonitor:
      enabled: true
      # interval: ""
      # additionalLabels: {}
      # annotations: {}

    serviceAccount:
      annotations: {} # TODO - add role
    config:
      # https://grafana.com/docs/loki/latest/configuration/examples/
      # schema_config:
      #   configs:
      #   - from: 2020-05-15
      #     store: aws
      #     object_store: s3
      #     schema: v11
      #     index:
      #       prefix: loki_index
      #       period: 0
      # storage_config:
      #   aws:
      #     s3: s3://us-east-2/loki-test-bucket-dev-east-2
      #     dynamodb:
      #       dynamodb_url: dynamodb://us-east-2
      table_manager:
        retention_deletes_enabled: true
        retention_period: 168h  # 1 week retention

  promtail:
    # https://github.com/grafana/helm-charts/blob/main/charts/promtail/values.yaml
    enabled: true
    # -- The security context for pods
    securityContext:

    # -- Resource requests and limits
    resources: {}

    image:
      # -- The Docker registry
      registry: docker.io
      # -- Docker image repository
      repository: grafana/promtail
      # -- Overrides the image tag whose default is the chart's appVersion - use 2.3.0
      tag: "2.3.0"
      # -- Docker image pull policy
      pullPolicy: IfNotPresent

    serviceMonitor:
      enabled: true
      interval: "60s"
      additionalLabels: {}
      annotations: {}
    # Check version of promtail chart - config changes a lot between 2.2.0 (ours) and latest
    # https://github.com/grafana/helm-charts/blob/promtail-2.2.0/charts/promtail/templates/configmap.yaml
    # https://github.com/grafana/helm-charts/blob/promtail-2.2.0/charts/promtail/values.yaml
    pipelineStages:
    - docker: {}
    # NGINX pipeline
    - match:
        selector: '{app_kubernetes_io_instance="ingress-nginx",app_kubernetes_io_name="ingress-nginx",container="controller"}'
        stages:
        - regex:
            expression: '^(?P<remote_addr>[\w\.]+) - (?P<remote_user>[^ ]*) \[(?P<time_local>.*)\] "(?P<method>[^ ]*) (?P<path>[^ \?]*)(?P<query_string>[^ ]*) (?P<protocol>[^ ]*)" (?P<status>[\d]+) (?P<body_bytes_sent>[\d]+) "(?P<http_referer>[^"]*)" "(?P<http_user_agent>[^"]*)"? (?P<request_length>[^ ]*)? (?P<request_time>[^ ]*)? \[(?P<proxy_upstream_name>.*)\] \[(?P<proxy_alternative_upstream_name>.*)\] (?P<upstream_addr>[^ ]*)? (?P<upstream_response_length>[^ ]*)? (?P<upstream_response_time>[^ ]*)? (?P<upstream_status>[^ ]*)? (?P<req_id>[^ ]*)?'
        - labels:
            #remote_addr:
            #remote_user:
            #time_local:
            method:
            path:
            protocol:
            status:
            #body_bytes_sent:
            #http_referer:
            #http_user_agent:
            proxy_upstream_name:
            upstream_addr:
            req_id:

    # CMS pipeline
    - match:
        selector: '{app_kubernetes_io_instance="cms-backend",container="cms-backend"}'
        stages:
        - json:
            expressions:
              level: L
              source_code: C
              timestamp: T
              method: method
              status: status
              type: type
              path: url
              entryTid: entryTid
              currTid: currTid
              extra:
        - labels:
            level:
            source_code:
            timestamp:
            method:
            status:
            type:
            path:
            entryTid:
            currTid:
        - drop:
            source: "path"
            expression: "/v1/ping"
            drop_counter_reason: "cms-backend-ping"
        - timestamp:
            format: RFC3339
            source: timestamp

    # Typescript/NodeJS log pipeline - use for backend services
    - match:
        selector: '{container="assessment|auth-backend|live-backend|user-service|xapi"}'
        stages:
        - multiline:
            # RE2 regular expression, match lines which don't begin with whitespace or "}"
            firstline: '^[^\s\}]+'
        - regex:
            # If an exception message, pull the error, stack trace, function name, source code and exception data out
            expression: '\s*(?P<error>(?s:[^\n])*)\s*(?P<stack_trace>(?s:.*))at (?P<function_name>[^ ]*)\s*(?P<function_alias>\[as .*\])?\s+\((?P<source_code>[^ ]*)\)\s*(?P<exc_data>(?s:\{.*\}))*'
        - labels:
            error:
            source_code:
            function_name:

    # SFU log pipeline
    - match:
        selector: '{app_kubernetes_io_instance="sfu",container="sfu"}'
        stages:
        - json:
            expressions:
              level: level
              source_code: caller
              msg: msg
              ts: ts
        - labels:
            level:
            source_code:
        - timestamp:
            format: Unix
            source: ts

    # SFU gateway log pipeline
    - match:
        selector: '{app_kubernetes_io_instance="sfu-gateway",container="sfu-gateway"}'
        stages:
        - drop:
            # Drop + count "request" message
            expression: "request"
            drop_counter_reason: "sfu-gateway-request"

    # Meta monitoring
    - match:
        selector: '{container=~"alertmanager|prometheus|prometheus-postgres-exporter|prometheus-redis-exporter|promtail|loki"}'
        stages:
        - regex:
            # Pull level
            expression: 'level=\"*(?P<level>([^\" ])+)\"*'
        - regex:
            # Pull source
            expression: 'caller=\"*(?P<source_code>([^\" ])+)\"*'
        - regex:
            # Pull source
            expression: '(time|ts)=\"*(?P<ts>([^\" ])+)\"*'
        - labels:
            level:
            source_code:
            function_name:
        - timestamp:
            format: RFC3339
            source: ts

  fluent-bit:
    enabled: false

  grafana:
    # not required - already included with kube-prometheus-stack
    enabled: false
    sidecar:
      datasources:
        enabled: true

  prometheus:
    # not required - already included with kube-prometheus-stack
    enabled: false
